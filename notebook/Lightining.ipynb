{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0b0388-3ddb-4651-882a-002b364177af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pytorch Lightining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f212bb13-0537-4e22-aa4c-608ed067dbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/neptune/internal/backends/hosted_client.py:48: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  from neptune.version import version as neptune_client_version\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/neptune.py:39: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  from neptune import new as neptune\n",
      "/opt/conda/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AsChannelFirst'>: Class `AsChannelFirst` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "from monai.transforms import (\n",
    "    AsChannelFirst,\n",
    "    Compose,\n",
    "    RandFlip,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    Resize\n",
    ")\n",
    "from monai.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def labeling(data) :\n",
    "    label = []\n",
    "    for path in data : \n",
    "        label.append(os.path.basename(os.path.dirname(path)))\n",
    "    return label\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [   AsChannelFirst(2),\n",
    "        Resize((256,256)),\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [   AsChannelFirst(2),\n",
    "        Resize((256,256)),\n",
    "            ]\n",
    ")\n",
    "    \n",
    "\n",
    "class Sequence_Dataset(torch.utils.data.Dataset) : \n",
    "    def __init__(self,image_files,label, transforms): \n",
    "        self.image_files = image_files\n",
    "        self.transforms = transforms\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,index) :\n",
    "        img = np.load(self.image_files[index])\n",
    "        return self.transforms(img), self.label[index]\n",
    "\n",
    "class DataModule(pl.LightningDataModule) : \n",
    "    \n",
    "    def __init__(self, data_dir : list , batch_size : int ) : \n",
    "        \n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.__ = None\n",
    "        \n",
    "    def prepare_data(self) :\n",
    "        y = labeling(X)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2,shuffle=True ,random_state=42,stratify=y)\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X_train, y_train, test_size=0.2,shuffle=True ,random_state=42,stratify=y_train)\n",
    "    \n",
    "    def setup(self,stage : str) : \n",
    "        \n",
    "        if stage == \"fit\" : \n",
    "            self.train_ds =  Sequence_Dataset(self.X_train,self.y_train,train_transforms)\n",
    "            self.val_ds = Sequence_Dataset(self.X_val,self.y_val,val_transforms)\n",
    "            \n",
    "        if stage == \"test\" : \n",
    "            self.test_ds = Sequence_Dataset(self.X_test,self.y_test,val_transforms)\n",
    "            \n",
    "    def train_dataloader(self) :\n",
    "        \n",
    "        return DataLoader(self.train_ds, batch_size  = self.batch_size, shuffle=True, num_workers=8 , pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self) :\n",
    "        \n",
    "        return DataLoader(self.val_ds, batch_size  = self.batch_size, shuffle=True, num_workers=8 , pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self) :\n",
    "        \n",
    "        return DataLoader(self.test_ds, batch_size  = self.batch_size, shuffle=True, num_workers=8 , pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a99d1ba-a102-4ffa-8a46-fcd989fa74d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from monai.utils import set_determinism\n",
    "import os\n",
    "set_determinism(seed=0)\n",
    "T1  = [\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/T1/\" + x for x in os.listdir(\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/T1/\")]\n",
    "T2  = [\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/T2/\" + x for x in os.listdir(\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/T2/\")]\n",
    "FLAIR  = [\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/FLAIR/\" + x for x in os.listdir(\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/FLAIR/\")]\n",
    "OTHER  = [\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/OTHER/\" + x for x in os.listdir(\"/home/jovyan/Verdicchio/ONWAY_DATA/Dataset/OTHER/\")]\n",
    "\n",
    "\n",
    "X = T1 + T2 + FLAIR +OTHER \n",
    "y = list(np.zeros(len(T1),dtype = int)) + list(np.ones(len(T2),dtype = int)) + list(2*np.ones(len(FLAIR),dtype = int)) + list(3*np.ones(len(OTHER),dtype = int))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,shuffle=True ,random_state=42,stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,shuffle=True ,random_state=42,stratify=y_train)\n",
    "\n",
    "dm = DataModule(X,1)\n",
    "dm.prepare_data()\n",
    "dm.setup(stage =\"fit\")\n",
    "A = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324ae7a-39b0-4efe-94d0-47296b35386b",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d221459-592c-44b4-87f6-ea7a97021ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model,metric,loss_function):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.metric = metric\n",
    "        \n",
    "    def forward(self, x) : \n",
    "        self.model(x)\n",
    "    \n",
    "    def prepare_batch(self,batch) : \n",
    "        return  batch[0].to(device), torch.tensor(batch[1]).to(device)\n",
    "    \n",
    "    def infer_batch(self, batch):\n",
    "        x, y = self.prepare_batch(batch)\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat, y\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        return optimizer, StepLR(optimizer, gamma = 0.1, step_size=25,verbose = True, last_epoch = 150)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs,labels = self.infer_batch(batch)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.epoch_loss_values.append(avg_loss.detach().cpu().numpy())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs,labels = self.infer_batch(batch)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
    "        labels = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
    "        self.metric(y_pred=outputs, y=labels)\n",
    "        return {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba777b-3f2c-4f59-9467-a2488a02718b",
   "metadata": {},
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7845a498-ea46-4d6d-9799-b32703604b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "model = resnet50(weights= None, num_classes=4,progress=True).to(device)\n",
    "class_weight = torch.tensor(compute_class_weight(\"balanced\",classes = np.unique(y_train),y=y_train ))\n",
    "loss_function = CrossEntropyLoss(weight= class_weight.float().to(device))\n",
    "metric = ROCAUCMetric\n",
    "net = Model(model,metric,loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af345249-d7ca-42f9-8339-00f62f627ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(net, save_name,CHECKPOINT_PATH, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        net - Model Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        # How many epochs to train for if no patience is set\n",
    "        max_epochs=180,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                save_weights_only=True, mode=\"max\", monitor=\"val_acc\"\n",
    "            ),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],  # Log learning rate every epoch\n",
    "    )  # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = net.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducable\n",
    "        model = net\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = net.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )  # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9394db-bea5-4395-87af-ec8c6688931c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py:236: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a parent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model,result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodello1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../checkpoint/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(net, save_name, CHECKPOINT_PATH, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    net - Model Is used to look up the class in \"model_dict\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a PyTorch Lightning trainer with the generation callback\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Where to save models\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# How many epochs to train for if no patience is set\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_acc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLearningRateMonitor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Log learning rate every epoch\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# In case your notebook crashes due to the progress bar, consider increasing the refresh rate\u001b[39;00m\n\u001b[1;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39m_log_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# If True, we plot the computation graph in tensorboard\u001b[39;00m\n\u001b[1;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39m_default_hp_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Optional logging argument that we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:348\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:466\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# init callbacks\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Declare attributes to be set in _callback_connector on_trainer_init\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trainer_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_model_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# init data flags\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch: Optional[\u001b[38;5;28mint\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:86\u001b[0m, in \u001b[0;36mCallbackConnector.on_trainer_init\u001b[0;34m(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time, accumulate_grad_batches)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fault_tolerance_callbacks()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mextend(_configure_external_callbacks())\n\u001b[0;32m---> 86\u001b[0m \u001b[43m_validate_callbacks_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# push all model checkpoint callbacks to the end\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# it is important that these are the last callbacks to run\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reorder_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:298\u001b[0m, in \u001b[0;36m_validate_callbacks_list\u001b[0;34m(callbacks)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_callbacks_list\u001b[39m(callbacks: List[Callback]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     stateful_callbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks \u001b[38;5;28;01mif\u001b[39;00m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m, instance\u001b[38;5;241m=\u001b[39mcb)]\n\u001b[1;32m    299\u001b[0m     seen_callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:298\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_callbacks_list\u001b[39m(callbacks: List[Callback]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     stateful_callbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_overridden\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    299\u001b[0m     seen_callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py:34\u001b[0m, in \u001b[0;36mis_overridden\u001b[0;34m(method_name, instance, parent)\u001b[0m\n\u001b[1;32m     32\u001b[0m         parent \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mCallback\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a parent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_utilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_overridden\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_overridden(method_name, instance, parent)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a parent"
     ]
    }
   ],
   "source": [
    "model,result = train_model(net,\"modello1\",CHECKPOINT_PATH = \"../checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d7702-3267-4574-81a4-0d4537ef3aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
